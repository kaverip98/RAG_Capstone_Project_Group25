{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preYTw7AG-7D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install these first if not installed (uncomment if needed)\n",
        "!pip install -U datasets sentence-transformers faiss-cpu langchain langchain-community tqdm groq regex gradio\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import regex as re\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwR-kNKDnYSp",
        "outputId": "c54c36dc-1c57-4d2d-f70e-4b037d7b02db",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Global variables ===\n",
        "stop_initialization = False\n",
        "vectorstore = None\n",
        "embeddings = None\n",
        "groq_model = None\n",
        "judge_model_settings = None\n",
        "chunk_settings = None\n",
        "top_k_settings = None\n",
        "client = None\n",
        "\n",
        "# === Prompts ===\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"Use the following context to answer:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "If the answer isn't in the context, say \"I don't know\".\"\"\"\n",
        "\n",
        "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
        "I asked someone to answer a question based on one or more documents.\n",
        "Your task is to review their response and assess whether or not each sentence\n",
        "in that response is supported by text in the documents. And if so, which\n",
        "sentences in the documents provide that support.\n",
        "\n",
        "Here are the documents, each split into sentences with keys like '0_0.', '0_1.':\n",
        "'''\n",
        "{context}\n",
        "'''\n",
        "\n",
        "The question was:\n",
        "'''\n",
        "{question}\n",
        "'''\n",
        "\n",
        "Here is their response, split into sentences with keys like 'a.', 'b.':\n",
        "'''\n",
        "{answer}\n",
        "'''\n",
        "\n",
        "You must respond with a JSON object matching this schema:\n",
        "{{\n",
        "  \"relevance_explanation\": string,\n",
        "  \"all_relevant_sentence_keys\": [string],\n",
        "  \"overall_supported_explanation\": string,\n",
        "  \"overall_supported\": boolean,\n",
        "  \"sentence_support_information\": [\n",
        "    {{\n",
        "      \"response_sentence_key\": string,\n",
        "      \"explanation\": string,\n",
        "      \"supporting_sentence_keys\": [string],\n",
        "      \"fully_supported\": boolean\n",
        "    }}\n",
        "  ],\n",
        "  \"all_utilized_sentence_keys\": [string]\n",
        "}}\n",
        "\n",
        "‚ö†Ô∏è IMPORTANT:\n",
        "- Strictly return ONLY the JSON object.\n",
        "- Do NOT add any text, comments, markdown, or code fences.\n",
        "- Output must start directly with '{{' and end with '}}'.\n",
        "\"\"\"\n",
        "\n",
        "# === Helper functions ===\n",
        "def check_stop():\n",
        "    global stop_initialization\n",
        "    if stop_initialization:\n",
        "        raise Exception(\"Initialization stopped by user.\")\n",
        "\n",
        "def unzip_if_needed(local_index_path):\n",
        "    zip_index_path = local_index_path + \".zip\"\n",
        "    if not os.path.exists(local_index_path) and os.path.exists(zip_index_path):\n",
        "        print(f\"Unzipping vectorstore from {zip_index_path}...\")\n",
        "        with zipfile.ZipFile(zip_index_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(local_index_path)\n",
        "\n",
        "def zip_vectorstore(local_index_path):\n",
        "    zip_index_path = local_index_path + \".zip\"\n",
        "    print(f\"Zipping vectorstore to {zip_index_path}...\")\n",
        "    with zipfile.ZipFile(zip_index_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(local_index_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, local_index_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    matches = re.findall(r'\\{(?:[^{}]|(?R))*\\}', text, flags=re.DOTALL)\n",
        "    for m in matches:\n",
        "        try:\n",
        "            json.loads(m)\n",
        "            return m\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def validate_evaluation(data):\n",
        "    defaults = {\n",
        "        \"relevance_explanation\": \"N/A\",\n",
        "        \"all_relevant_sentence_keys\": [],\n",
        "        \"overall_supported_explanation\": \"N/A\",\n",
        "        \"overall_supported\": False,\n",
        "        \"sentence_support_information\": [],\n",
        "        \"all_utilized_sentence_keys\": []\n",
        "    }\n",
        "    for k, v in defaults.items():\n",
        "        if k not in data:\n",
        "            data[k] = v\n",
        "    return data\n",
        "\n",
        "def calculate_metrics(evaluation):\n",
        "    try:\n",
        "        relevant = len(evaluation.get(\"all_relevant_sentence_keys\", []))\n",
        "        utilized = len(evaluation.get(\"all_utilized_sentence_keys\", []))\n",
        "        supported = sum(1 for s in evaluation.get(\"sentence_support_information\", []) if s.get(\"fully_supported\"))\n",
        "        total = max(1, len(evaluation.get(\"sentence_support_information\", [])))\n",
        "        return {\n",
        "            \"context_relevance\": relevant / max(1, utilized),\n",
        "            \"context_utilization\": utilized / max(1, relevant),\n",
        "            \"completeness\": supported / total,\n",
        "            \"adherence\": 1.0 if evaluation.get(\"overall_supported\") else 0.0,\n",
        "            \"explanation\": f\"Relevant:{relevant}, Utilized:{utilized}, Supported:{supported}/{total}\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Metric calc error: {e}\")\n",
        "        return {\"context_relevance\": 0, \"context_utilization\": 0, \"completeness\": 0, \"adherence\": 0, \"explanation\": \"Failed\"}\n",
        "\n",
        "def format_context(docs):\n",
        "    lines = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        for j, s in enumerate(doc[\"content\"].split('. ')):\n",
        "            if s.strip():\n",
        "                lines.append(f\"{i}_{j}. {s.strip()}\")\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "# === Groq call ===\n",
        "def call_groq_model(prompt, model, temperature=0.3, max_tokens=512):\n",
        "    global client\n",
        "    if not client:\n",
        "        return None\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Groq call error: {e}\")\n",
        "        return None\n",
        "\n",
        "# === Initialize system ===\n",
        "def initialize_system(database, vectordb, embedding_model, retriever_model, judge_model, chunk_size, chunk_overlap, top_k):\n",
        "    global vectorstore, embeddings, groq_model, judge_model_settings, chunk_settings, top_k_settings, stop_initialization\n",
        "    stop_initialization = False\n",
        "\n",
        "    groq_model = retriever_model\n",
        "    judge_model_settings = judge_model\n",
        "    chunk_settings = (chunk_size, chunk_overlap)\n",
        "    top_k_settings = top_k\n",
        "\n",
        "    # Auto-select embedding model\n",
        "    if database == \"Medical\":\n",
        "        ragdatasets = [\"covidqa\", \"pubmedqa\"]\n",
        "        embedding_model = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
        "    else:\n",
        "        ragdatasets = [\"expertqa\", \"hotpotqa\", \"msmarco\", \"hagrid\"] if database == \"General Knowledge\" else \\\n",
        "                      [\"finqa\", \"tatqa\"] if database == \"Finance\" else \\\n",
        "                      [\"delucionqa\", \"emanual\", \"techqa\"]\n",
        "        embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "    try:\n",
        "        datasets_list = []\n",
        "        for d in ragdatasets:\n",
        "            check_stop()\n",
        "            datasets_list.append(load_dataset(\"rungalileo/ragbench\", d, split=\"test\"))\n",
        "        datasets = concatenate_datasets(datasets_list)\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        all_docs, chunks = [], []\n",
        "        for row in datasets:\n",
        "            check_stop()\n",
        "            for doc in row['documents']:\n",
        "                if doc.strip():\n",
        "                    all_docs.append(doc)\n",
        "\n",
        "        for text in all_docs:\n",
        "            check_stop()\n",
        "            chunks.extend(text_splitter.split_text(text))\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=embedding_model,\n",
        "            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "            encode_kwargs={\"batch_size\": 32, \"normalize_embeddings\": True}\n",
        "        )\n",
        "\n",
        "        local_index_path = f\"{vectordb.lower()}_index_{database.lower().replace(' ', '_')}\"\n",
        "        unzip_if_needed(local_index_path)\n",
        "\n",
        "        if vectordb == \"FAISS\":\n",
        "            if os.path.exists(local_index_path):\n",
        "                vectorstore = FAISS.load_local(local_index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "            else:\n",
        "                vectorstore = FAISS.from_texts(chunks, embeddings)\n",
        "                vectorstore.save_local(local_index_path)\n",
        "                zip_vectorstore(local_index_path)\n",
        "        elif vectordb == \"Chroma\":\n",
        "            if os.path.exists(local_index_path):\n",
        "                vectorstore = Chroma(\n",
        "                    persist_directory=local_index_path,\n",
        "                    embedding_function=embeddings\n",
        "                )\n",
        "            else:\n",
        "                vectorstore = Chroma.from_texts(\n",
        "                    texts=chunks,\n",
        "                    embedding=embeddings,\n",
        "                    persist_directory=local_index_path\n",
        "                )\n",
        "                vectorstore.persist()\n",
        "                zip_vectorstore(local_index_path)\n",
        "        else:\n",
        "            return \"Error: Invalid vector database selection\", None\n",
        "    except Exception as e:\n",
        "        return f\"Initialization error: {str(e)}\", None\n",
        "\n",
        "    return f\"System initialized with {database}, {vectordb}, embedding: {embedding_model}\", f\"{len(chunks)} chunks created\"\n",
        "\n",
        "# === Process query ===\n",
        "def evaluate_answer(question, context, answer):\n",
        "    prompt = JUDGE_PROMPT_TEMPLATE.format(question=question, context=context, answer=answer)\n",
        "    output = call_groq_model(prompt, judge_model_settings, temperature=0.1)\n",
        "    if not output:\n",
        "        return None\n",
        "    json_str = extract_json_from_text(output)\n",
        "    if not json_str:\n",
        "        return None\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "        return validate_evaluation(data)\n",
        "    except Exception as e:\n",
        "        print(f\"JSON parse error: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_query(question, temperature=0.3, max_tokens=512):\n",
        "    if not vectorstore:\n",
        "        return \"Error: System not initialized.\", None, None, None\n",
        "    try:\n",
        "        docs = vectorstore.similarity_search(question, k=top_k_settings)\n",
        "        formatted = [{\"content\": d.page_content} for d in docs]\n",
        "        context = \"\\n\\n\".join(d[\"content\"] for d in formatted)\n",
        "\n",
        "        prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n",
        "        answer = call_groq_model(prompt, groq_model, temperature, max_tokens) or \"I don't know\"\n",
        "\n",
        "        judge_context = format_context(formatted)\n",
        "        eval_result = evaluate_answer(question, judge_context, answer)\n",
        "        metrics = calculate_metrics(eval_result) if eval_result else {\"context_relevance\":0,\"context_utilization\":0,\"completeness\":0,\"adherence\":0,\"explanation\":\"Evaluation failed\"}\n",
        "\n",
        "        retrieved_docs = \"\\n\\n\".join([f\"Document {i+1}:\\n{d['content']}\" for i, d in enumerate(formatted)])\n",
        "        eval_output = json.dumps(eval_result, indent=2) if eval_result else \"Evaluation failed\"\n",
        "        metrics_output = json.dumps(metrics, indent=2)\n",
        "\n",
        "        return answer, retrieved_docs, eval_output, metrics_output\n",
        "    except Exception as e:\n",
        "        return f\"Query error: {str(e)}\", None, None, None\n"
      ],
      "metadata": {
        "id": "eiwiOsgqnKPj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Gradio interface ===\n",
        "DATABASE_OPTIONS = [\"Medical\", \"Finance\", \"General Knowledge\", \"Customer Support\"]\n",
        "VECTORDB_OPTIONS = [\"FAISS\", \"Chroma\"]\n",
        "EMBEDDING_MODELS = [\"sentence-transformers/all-MiniLM-L6-v2\", \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"]\n",
        "LLM_MODELS = [\"llama3-8b-8192\", \"llama3-70b-8192\", \"mixtral-8x7b-32768\"]\n",
        "\n",
        "# --- Groq client ---\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "client = Groq(api_key=groq_api_key)\n",
        "\n",
        "with gr.Blocks(title=\"RAG System\") as demo:\n",
        "    gr.Markdown(\"# üß† RAG System with Groq\")\n",
        "\n",
        "    with gr.Tab(\"Configuration\"):\n",
        "        database = gr.Dropdown(label=\"Knowledge Domain\", choices=DATABASE_OPTIONS, value=DATABASE_OPTIONS[0])\n",
        "        vectordb = gr.Dropdown(label=\"Vector Database\", choices=VECTORDB_OPTIONS, value=VECTORDB_OPTIONS[0])\n",
        "        embedding_model = gr.Dropdown(label=\"Embedding Model (auto-selected)\", choices=EMBEDDING_MODELS, value=EMBEDDING_MODELS[0])\n",
        "        retriever_model = gr.Dropdown(label=\"Retriever LLM\", choices=LLM_MODELS, value=LLM_MODELS[0])\n",
        "        judge_model = gr.Dropdown(label=\"Judge LLM\", choices=LLM_MODELS, value=LLM_MODELS[1])\n",
        "        chunk_size = gr.Number(label=\"Chunk Size\", value=500)\n",
        "        chunk_overlap = gr.Number(label=\"Chunk Overlap\", value=100)\n",
        "        top_k = gr.Number(label=\"Top K Documents\", value=5)\n",
        "        init_btn = gr.Button(\"‚úÖ Initialize System\")\n",
        "        stop_btn = gr.Button(\"üõë Stop Initialization\")\n",
        "        init_output = gr.Textbox(label=\"Initialization Status\", interactive=False)\n",
        "        chunk_info = gr.Textbox(label=\"Chunk Information\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"Query\"):\n",
        "        question = gr.Textbox(label=\"Your question\")\n",
        "        temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=1.0, step=0.1, value=0.3)\n",
        "        max_tokens = gr.Slider(label=\"Max Tokens\", minimum=128, maximum=2048, step=64, value=512)\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "        answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "        docs_output = gr.Textbox(label=\"Retrieved Documents\", interactive=False, lines=10)\n",
        "        eval_output = gr.Textbox(label=\"Evaluation Results\", interactive=False, lines=10)\n",
        "        metrics_output = gr.Textbox(label=\"Metrics\", interactive=False, lines=5)\n",
        "\n",
        "    # Events\n",
        "    def stop_init(): global stop_initialization; stop_initialization=True; return \"Stopping initialization...\"\n",
        "\n",
        "    init_btn.click(\n",
        "        initialize_system,\n",
        "        inputs=[database, vectordb, embedding_model, retriever_model, judge_model, chunk_size, chunk_overlap, top_k],\n",
        "        outputs=[init_output, chunk_info]\n",
        "    )\n",
        "    stop_btn.click(stop_init, inputs=[], outputs=[init_output])\n",
        "    submit_btn.click(\n",
        "        process_query,\n",
        "        inputs=[question, temperature, max_tokens],\n",
        "        outputs=[answer_output, docs_output, eval_output, metrics_output]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "InSbHczknRft"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "piVGulX8nLY7",
        "outputId": "dd68376f-19c2-4950-bfe4-5fe0b59e1d58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4aa098823b725faaaf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4aa098823b725faaaf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}